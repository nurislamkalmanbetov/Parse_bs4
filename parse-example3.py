# Этот модуль помогает распарсить html и получать те элементы страницы информации которые нам нужны
from bs4 import BeautifulSoup
import requests
import csv


# Добавляю переменную хост для передачи домена имени в данном случае  хост это kivano.kg указываю его в виде строки
HOST = 'https://www.kivano.kg/'
# Для чего нужен хост и для чего нужен url  для того чтобы
# Теперь мне нужен урл который будет указывается для конкретики страницы где будет спарсена данные
URL = 'https://www.kivano.kg/planshety'
# И указываю переменную csv для того чтобы все наши данные которые мы хотим спарсить были в добавлены в формате таблицы
CSV = 'kivano_products.csv'
HEADERS = {  # Для того чтобы мы делали запросы на сайты которые мы хотим получать информацию  и
    #  чтобы не подумали что это какой нибудь скрипт или бот  а чтобы подумали что мы реальный человоек должны указывать заголовки
    #   для того чтобы взять заголовок нам нужно перейти в сайт и нажать CTRL+shift+c  или правая кнопка мыши просмотреть код и переходим в раздел СЕТЬ b получаю информацию о ACCEPT и USERS
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    # HTTP заголовок запроса Accept указывает, какие типы контента, выраженные как MIME типы, клиент может понять.
    #  Используя согласование контента, сервер затем выбирает одно из предложений, использует его и информирует клиента о своём выборе с помощью заголовка ответа Content-Type
    'User-Agen :Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:90.0) Gecko/20100101 Firefox/90.0'
    # Для того чтобы указать пользователя который просматривает информацию
}

# Теперь пропишим функцию которая будет получать HTML и пробрасываем url
#  пишем параметры которые будут принимать по умолчанию пока что у нас нету параметров и мы указываем что он пока пуст


def get_html(url, params=''):
    # И теперь используем модуль request для запроса на страницу  и будем забирать HTML и обращаемся к методу библиотеки get() и пробрасываем  url и заголовки
    # для обращение к другому сайту  и указываем какие  параметры мы будем передавать обязательные и необезательные
    r = requests.get(url, headers=HEADERS, params=params)
    return r
    # и возвращаем нашу r

    # Давайте проверим ! Укажем переменную и в нем передадим нашу функцию где будет содержатся URL страницы
# html = get_html(URL)

# И принтуем нашу переменную которая теперь является аргументом функции
# print(html)
# И получаем 200 ответ это означает что дейсвительно html взят

# Давайте создадим функцию в эту функцию будем пробрасывать  html с которым мы будем дальше работать


def get_content(html):
    # и теперь давай  активируем наш модуль beatifulsoup  у которого есть определенные параметры это html и 'html.parser' которая будет вытягивать уже определенные данные
    soup = BeautifulSoup(html, 'html.parser')
    # И теперь нам нужны из полученных html получать определенные элементы теперь мы используем через инспектор определенный блок указыаем его и по кусочкам
    # разбтирать его
    #  Для того чтобы приципица к определенный блок и у них у каждого есть класс внутри div  и в переменной items и в нее будем парсить определеные данные
    #  И чтобы получить много таких данных мы используем findAll и указыаем внутри конкретику видим что есть элемент 'div' и внутри есть класс который называется ''
    items = soup.findAll('div', class_='item product_listbox oh')
    # И давайте передадим пустой лист Далее функции пропишем цикл который будет пробегаться по этим items
    planshet_list = []
    # и давайте покажу что мы получили
    # print(items)
# html = get_html(URL)
# чтобы получить код содержимого а не ответ  200 то укажем его
# get_content(html.text)
# и теперь пропишем цикл списков всех item  разбивать по одному items
    for item in items:
        # и теперь нам нужно каждый наш элемент добавить  в наш пустой список и укажем что будем добавлять массив
        planshet_list.append({
            #  и давайте как мы говорили будем разбивать по кусочкам и собрать его
            #   и в каждом кусочке был название цена описание и так далее и с помощью функцию get_text()получать элементы
            'title': item.find('div', class_='listbox_title oh').get_text(strip=True),
            # и таким образом я хочу получить описание мы указываем другой тег и  название класса
            # и укажу чтобы пробелов при добавлении не было
            'product_text': item.find('div', class_='product_text').get_text(strip=True),
            'price': item.find('div', class_='listbox_price').get_text(strip=True),
            # и добавим ссылку и тут внутри нее ищем определенный значение указыаем другое название и используем href для передачи ссылки
            'link': item.find('div', class_='listbox_title oh').find('a').get('href'),
        })
    return planshet_list
# Возращаем наш список
#  пишу функцию добавления в csv файл


def planshet_save(items, path):
    with open(path, 'w') as file:
        writer = csv.writer(file, delimiter=',')
        writer.writerow(['Наименование', 'Описание', 'Стоимость', 'Ссылка'])
        for items in items:
            writer.writerow(
                [items['title'], items['product_text'], items['price'], items['link']])

# и давайте напишем функцию которая будет брать все данные и будет его правильно формировать


def parse():
    # и я хочу чтобы пользователь указывал сколько количество ему добавлять и указать чтобы пробелов не было
    PAGENATION = input("Введите количество страниц: ")
# и указать чтобы пробелов не было
    PAGENATION = int(PAGENATION.strip())
    html = get_html(URL)
    if html.status_code == 200:  # и здесь происходит процесс добавления
        planshet_list = []
# и указываю цикл прибавления
        for page in range(1, PAGENATION):
            # и чтобы в консоле я получал значение
            print(f'Страница №{page} готова')
# и пишу чтобы все страницы были в параметрах и добавляли
            html = get_html(URL, params={'page': page})
# ипользуем екстенд он повторяется и каждый элемент итерации добавлялся
            planshet_list.extend(get_content(html.text))
# и наша функцкия добавления
        planshet_save(planshet_list, CSV)
# и в консоле получу когда мой парсер закончится
        print('Парсинг готов')
    else:
        #  или же получу else
        print('Error')


parse()